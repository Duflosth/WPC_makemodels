import numpy as np

def neural_network_gyr(values):
  
  values = np.array(values)
  def col(M,i):
    return([row[i] for row in M])

  def transpose(M):
    rows, cols = M.shape
    tM = np.zeros((cols, rows))
    for i in range(rows):
       for j in range(cols):
          tM[j,i] = M[i,j]
    return tM
          
  def print_matrice(M, decimals = 3):
    """
    Print a matrix one row at a time
        :param M: The matrix to be printed
        """
    for row in M:
        print([round(x, decimals) for x in row])

  def relu(x):
    return np.maximum(0,x)

  def sigmoid(x):
    ## the possible range of numbers in micropython is severely constrained
    return 1 / (1 + np.exp(-x))


  def dense(nunit, x, w, b, activation): #a single dense layer with activation
    res = neuron(x, w[0], b[0], activation)
    for i in range(1,nunit):
        z = neuron(x, w[i], b[i], activation)
        res = np.concatenate((res,z))
    return res.reshape((nunit,-1))

  def neuron(x, w, b, activation): #perfor operation on a singke neuron and return a 1d array
    z = np.dot(w,x) + b
    if activation == "sigmoid":
        yp = sigmoid(z)
    elif activation == "relu":
        yp = relu(z)
    # elif activation == "softmax":
    #     yp = softmax([tmp[i] + b for i in range(len(tmp))])  
    else:
        print("Invalid activation function -- Please choose between 'relu', 'sigmoid' or 'softmax")
    return yp      

  w1 = np.array([[-0.1648489 ,  0.09379578,  0.0882242 ,  0.18258762,  0.00844538,
         0.31868744, -0.00105914,  0.16615862],
       [ 0.1491864 ,  0.3033024 ,  0.11609706, -0.1158146 , -0.1732272 ,
         0.28616118, -0.1374421 ,  0.20114368],
       [ 0.18465161, -0.33349118, -0.22840676,  0.2911231 ,  0.13519216,
         0.29676372, -0.143431  , -0.24090889],
       [ 0.14455622,  0.09441826, -0.16161893, -0.24651235, -0.0855695 ,
         0.02683872, -0.23560435, -0.13387121],
       [ 0.31084484,  0.26566815, -0.3081992 ,  0.27938282,  0.07992533,
         0.3015771 ,  0.21118009, -0.04688537],
       [ 0.14922291,  0.10728884, -0.3198248 , -0.28603858,  0.12589559,
        -0.32686955,  0.07545528,  0.32940823],
       [-0.0633949 ,  0.03103077,  0.05211443, -0.013751  , -0.3244038 ,
        -0.29682976,  0.18315089, -0.05523026],
       [-0.21576832, -0.00513184, -0.2299715 , -0.15605976,  0.15956128,
        -0.14580084, -0.19529064, -0.3185467 ],
       [-0.26450852, -0.32843977,  0.06706291, -0.19925828,  0.1520285 ,
        -0.26637483,  0.2055493 , -0.15458138],
       [-0.1264459 ,  0.33841962,  0.05962077,  0.27492458,  0.32004714,
        -0.00320831,  0.09758171, -0.09502479],
       [ 0.00448543, -0.00890616, -0.1101051 , -0.1833667 ,  0.0194267 ,
         0.33442634, -0.3412275 , -0.16868052],
       [ 0.03769031,  0.31032854, -0.3074808 ,  0.20973808,  0.28974473,
         0.07805219, -0.04345655, -0.20562883],
       [ 0.04234701,  0.06238836, -0.33349422, -0.0588606 ,  0.15918696,
         0.1579169 ,  0.09107316, -0.3131946 ],
       [-0.02020118, -0.3247216 ,  0.02775508, -0.28970706,  0.34188855,
         0.28613132, -0.2784981 ,  0.28842235],
       [-0.2855816 , -0.18886764,  0.32115203,  0.10232565,  0.30626923,
        -0.13444662, -0.15788203,  0.2508362 ],
       [ 0.06819409,  0.2853335 , -0.19225246,  0.22111374, -0.32483914,
        -0.08222705,  0.13496941, -0.3232733 ],
       [-0.3449251 , -0.14680755,  0.2938233 ,  0.2329647 , -0.28650934,
         0.1276561 , -0.23571147,  0.30288076],
       [ 0.2005434 ,  0.11000252, -0.1573827 ,  0.16989684, -0.13292448,
        -0.10966977,  0.31463075, -0.16911411],
       [-0.01256064, -0.04485455, -0.20644905,  0.19018024,  0.03536084,
         0.3125121 , -0.09774409, -0.3206963 ],
       [ 0.19936311,  0.16980094, -0.27180663, -0.10315353,  0.17751122,
        -0.30322105, -0.07362515, -0.3224189 ],
       [ 0.1327816 ,  0.10410738, -0.27605608,  0.06910399, -0.05773413,
         0.2818727 ,  0.33877546, -0.31118724],
       [ 0.26043177,  0.09481412,  0.32596135, -0.31092426, -0.09681594,
        -0.09391239, -0.18892282, -0.09110415],
       [-0.2870337 ,  0.1767878 ,  0.2936926 ,  0.06809846,  0.34215593,
        -0.07805061,  0.05085468, -0.03716892],
       [ 0.34575927,  0.1328865 ,  0.20474511, -0.22876444,  0.2900557 ,
        -0.3218966 ,  0.0994775 , -0.29047227],
       [ 0.16227561, -0.24617282,  0.22973812,  0.18635291, -0.0907822 ,
        -0.11367376, -0.03345978, -0.09503421],
       [ 0.32710963, -0.0669525 ,  0.20562196, -0.25147298,  0.0633437 ,
         0.1479525 , -0.15629333,  0.07706425],
       [ 0.28789538,  0.21646726, -0.27430797,  0.22348857,  0.19151258,
        -0.14211896, -0.00280181, -0.26389387],
       [-0.06117865,  0.12842691, -0.13018197,  0.0291687 ,  0.02906984,
         0.10212478,  0.22599739,  0.26641434],
       [ 0.1687873 , -0.08499366,  0.05619112, -0.14613336,  0.1560278 ,
         0.06214845,  0.09542173, -0.08303678],
       [-0.28117588,  0.15428078, -0.09944695,  0.33028114,  0.09032565,
        -0.0054622 ,  0.2624184 ,  0.25865507],
       [-0.08268619,  0.21879113, -0.1927248 ,  0.12742385, -0.25640422,
        -0.08656594, -0.13794318, -0.23306239],
       [ 0.28311336, -0.06051961, -0.14002174, -0.08225554,  0.21609288,
         0.3332283 , -0.06337565, -0.14264713],
       [-0.12793253,  0.0931187 ,  0.3456363 , -0.02726457, -0.2387898 ,
         0.20302624, -0.13067372, -0.04585135],
       [-0.07919481, -0.08614787,  0.18225896, -0.23291239, -0.1716428 ,
         0.14814484, -0.15352298,  0.25209934],
       [-0.14047293, -0.08464605, -0.07391852,  0.15789866, -0.22413903,
         0.02799377,  0.13033634,  0.3036533 ],
       [ 0.20422441,  0.17413116, -0.31480297,  0.1861139 ,  0.15491372,
        -0.3375254 ,  0.3449859 ,  0.03869006],
       [ 0.07079899, -0.24986577,  0.3297447 , -0.04903951, -0.18652216,
        -0.25307104, -0.19579576,  0.3180334 ],
       [ 0.33607244,  0.27583265,  0.16655618, -0.10323736,  0.10978359,
        -0.05971202, -0.1371276 , -0.2700287 ],
       [-0.00518346, -0.17394872,  0.3227523 , -0.2721076 , -0.15431057,
         0.3050663 , -0.23034722, -0.2571501 ],
       [ 0.16429031, -0.2990622 , -0.31144145, -0.16280329,  0.09077814,
        -0.09607667,  0.01116824,  0.04541007],
       [-0.30777723, -0.16614102, -0.11262693,  0.01369402,  0.33205813,
         0.04278475,  0.18459111,  0.15112975],
       [-0.13699305, -0.04147214,  0.14532974, -0.1851348 ,  0.33236295,
        -0.09555924,  0.11737895,  0.06770161]])

  b1 = np.array([0., 0., 0., 0., 0., 0., 0., 0.])

  w2 = np.array([[ 0.16935664, -0.55173886, -0.5668869 ,  0.4103338 ],
       [-0.0236395 ,  0.52459425,  0.20620716,  0.12048352],
       [-0.35089913,  0.21216303,  0.33194417, -0.02701443],
       [ 0.47291476, -0.5869628 , -0.27229476, -0.65951085],
       [ 0.34770936, -0.654919  ,  0.7032154 , -0.43936318],
       [-0.04829913, -0.5909489 ,  0.20783538, -0.13857174],
       [-0.52886015,  0.34942943, -0.69998413,  0.08099663],
       [-0.19393146, -0.65969205,  0.42716795,  0.6144579 ]])

  b2 = np.array([0., 0., 0., 0.])
  
  w1 = transpose(w1)
  w2 = transpose(w2)

  yout1 = dense(8, transpose(values), w1, b1, 'relu') #input layer with 4 neuron
  ypred = dense(4, yout1, w2, b2,'sigmoid') #output layer

  return(np.argmax(ypred))