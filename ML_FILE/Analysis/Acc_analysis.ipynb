{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.basename(os.getcwd()) != \"ML_FILE\":\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_concat(List_df):\n",
    "    df = pd.DataFrame()\n",
    "    def lil_concat(a,b):\n",
    "        if a.empty:\n",
    "            return b\n",
    "        else:\n",
    "            b['ID'] += a['ID'].max()\n",
    "            b['index'] += a['index'].max() + 1\n",
    "            return (pd.concat([a,b], axis = 0, ignore_index=True))\n",
    "    for i in range(len(List_df)):\n",
    "        df = lil_concat(df, List_df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Loads accelerations data_set - adds yours data files here to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC1 = pd.read_csv('data_sensor/test_acc_C1_V0.csv')\n",
    "dfC2 = pd.read_csv('data_sensor/test_acc_C2_V0.csv')\n",
    "dfC3 = pd.read_csv('data_sensor/test_acc_C3_V0_vibrations.csv')\n",
    "#add yours data files below \n",
    "\n",
    "#example: type newdf = pd.read_csv('data_sensor/newdf_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this part add orientation data, and label them as neutral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf_ori = pd.read_csv('data_sensor/test_ori_C1_V0.csv')\n",
    "pitch_row = pd.Series(alldf_ori[alldf_ori.Type == 'Pitch'].ID.unique()).sample(n = 100)\n",
    "pitch_row = alldf_ori[alldf_ori.ID.isin(pitch_row)]\n",
    "\n",
    "roll_row = pd.Series(alldf_ori[alldf_ori.Type == 'Roll'].ID.unique()).sample(n = 100)\n",
    "roll_row = alldf_ori[alldf_ori.ID.isin(roll_row)]\n",
    "\n",
    "yaw_row = pd.Series(alldf_ori[alldf_ori.Type == 'Yaw'].ID.unique()).sample(n = 100)\n",
    "yaw_row = alldf_ori[alldf_ori.ID.isin(yaw_row)]\n",
    "\n",
    "dfneutral_ori = pd.concat([pitch_row, roll_row, yaw_row])\n",
    "dfneutral = pd.read_csv('data_sensor/test_neutral.csv')\n",
    "dfneutral = perfect_concat([alldf_ori, dfneutral])\n",
    "dfneutral['Type'] = 'Neutral'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Get the whole dataset, please modify list data below if you want to add your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListData = [dfC1, dfC2, dfC3, dfneutral]\n",
    "#example : do ListData.append(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldf = perfect_concat(ListData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Remove Gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max index : 26274\n"
     ]
    }
   ],
   "source": [
    "print(f\" Max index : {alldf.index.max()}\")\n",
    "DEG_TO_RAD = np.pi / 180\n",
    "\n",
    "alldf['X_acc'] = alldf['X_acc'] - 9.81* np.sin(DEG_TO_RAD * alldf['Pitch'])\n",
    "alldf['Y_acc'] = alldf['Y_acc'] - 9.81 * np.sin(DEG_TO_RAD * alldf['Roll'])*np.cos(DEG_TO_RAD * alldf['Pitch'])\n",
    "alldf['Z_acc'] = alldf['Z_acc'] - 9.81 * np.cos(DEG_TO_RAD * alldf['Roll'])*np.cos(DEG_TO_RAD * alldf['Pitch'])\n",
    "\n",
    "all_data = alldf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Look on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare number of measure by Type, neutral class is not perfectly balanced but it is normally sufficient since this class is not as important as others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '329'), Text(0, 0, '467'), Text(0, 0, '467'), Text(0, 0, '466')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = nb_by_type = all_data.groupby('Type').ID.nunique().plot.barh()\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabularize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the number of previous sample considered with length_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_tab = 7\n",
    "n_id = alldf[alldf['Type'] == 'X_acc'].ID.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Type', 'Interval Time', 'Time', 'index', 'X_acc_1', 'Y_acc_1', 'Z_acc_1', 'Roll_1', 'Pitch_1', 'Yaw_1', 'X_acc_2', 'Y_acc_2', 'Z_acc_2', 'Roll_2', 'Pitch_2', 'Yaw_2', 'X_acc_3', 'Y_acc_3', 'Z_acc_3', 'Roll_3', 'Pitch_3', 'Yaw_3', 'X_acc_4', 'Y_acc_4', 'Z_acc_4', 'Roll_4', 'Pitch_4', 'Yaw_4', 'X_acc_5', 'Y_acc_5', 'Z_acc_5', 'Roll_5', 'Pitch_5', 'Yaw_5', 'X_acc_6', 'Y_acc_6', 'Z_acc_6', 'Roll_6', 'Pitch_6', 'Yaw_6', 'X_acc_7', 'Y_acc_7', 'Z_acc_7', 'Roll_7', 'Pitch_7', 'Yaw_7']\n"
     ]
    }
   ],
   "source": [
    "columns = ['ID', 'Type', 'Interval Time', 'Time', 'index']\n",
    "for i in range(length_tab):\n",
    "    columns.append(f\"X_acc_{i+1}\")\n",
    "    columns.append(f\"Y_acc_{i+1}\")\n",
    "    columns.append(f\"Z_acc_{i+1}\")\n",
    "    columns.append(f\"Roll_{i+1}\")\n",
    "    columns.append(f\"Pitch_{i+1}\")\n",
    "    columns.append(f\"Yaw_{i+1}\")\n",
    "print(columns)\n",
    "wide_tab = pd.DataFrame(columns = columns)\n",
    "\n",
    "lil_tab = alldf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. We create the flipper feature : equal to 1 when the ID changes, and zero in the other case\n",
    "\\\n",
    "\"index_start\" represent the index for a same ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_tab = lil_tab.sort_values(by = ['ID', 'Time'], ascending = [True, True])\n",
    "lil_tab['flipper'] = np.where(lil_tab.ID != lil_tab.ID.shift(1), 1, 0)\n",
    "\n",
    "index_flipper = lil_tab[lil_tab['flipper'] == 1]\n",
    "index_flipper = index_flipper[['ID', 'index']]\n",
    "index_flipper.rename({'index' : 'index_start'}, axis = 1, inplace = True)\n",
    "lil_tab0 = lil_tab.merge(index_flipper, on = 'ID', how = 'left')\n",
    "lil_tab0['index_start'] = lil_tab0['index'] - lil_tab0['index_start'] #not +1 because I don't want to consider the \n",
    "#first value of each sample\n",
    "lil_tab0['lets_tab'] = np.where((lil_tab0.index_start >= length_tab), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Let's define the \"tabularized\" tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(length_tab):\n",
    "    wide_tab[f\"X_acc_{i+1}\"] = lil_tab[\"X_acc\"].shift(i)\n",
    "    wide_tab[f\"Y_acc_{i+1}\"] = lil_tab[\"Y_acc\"].shift(i)\n",
    "    wide_tab[f\"Z_acc_{i+1}\"] = lil_tab[\"Z_acc\"].shift(i)\n",
    "    wide_tab[f\"Roll_{i+1}\"] = lil_tab[\"Roll\"].shift(i)\n",
    "    wide_tab[f\"Pitch_{i+1}\"] = lil_tab[\"Pitch\"].shift(i)\n",
    "    wide_tab[f\"Yaw_{i+1}\"] = lil_tab[\"Yaw\"].shift(i)\n",
    "    \n",
    "wide_tab[\"ID\"] = lil_tab[\"ID\"]\n",
    "wide_tab[\"Type\"] = lil_tab[\"Type\"]\n",
    "wide_tab[\"Interval Time\"] = lil_tab[\"Interval Time\"]\n",
    "wide_tab[\"Time\"] = lil_tab[\"Time\"]\n",
    "wide_tab[\"index\"] = lil_tab0[\"index_start\"] - length_tab\n",
    "wide_tab[\"lets_tab\"] = lil_tab0[\"lets_tab\"]\n",
    "wide_tab = wide_tab[wide_tab[\"lets_tab\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. wide_tab_diff does only the differenciate of non orienation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_tab_diff = wide_tab.copy()\n",
    "\n",
    "wide_tab_diff['lets_diff'] = np.where(wide_tab_diff.ID != wide_tab_diff.ID.shift(1), 0, 1)\n",
    "for i in range(length_tab):\n",
    "    wide_tab_diff[f\"X_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'X_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Y_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Y_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Z_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Z_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Roll_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Roll_{i+1}'].diff(), 0)\n",
    "    wide_tab_diff[f\"Pitch_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Pitch_{i+1}'].diff(), 0)\n",
    "    wide_tab_diff[f\"Yaw_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Yaw_{i+1}'].diff(), 0)\n",
    "\n",
    "    wide_tab_diff.rename({f\"Roll_{i+1}\" : f\"diff_Roll_{i+1}\"}, axis = 1, inplace = True)\n",
    "    wide_tab_diff.rename({f\"Pitch_{i+1}\" : f\"diff_Pitch_{i+1}\"}, axis = 1, inplace = True)\n",
    "    wide_tab_diff.rename({f\"Yaw_{i+1}\" : f\"diff_Yaw_{i+1}\"}, axis = 1, inplace = True)\n",
    "wide_tab_diff = wide_tab_diff[wide_tab_diff[\"lets_diff\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5.The split_train_test function is a special : it does the split on the ID and then consider all samples of the same ID to be in a same train or test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_train_test(dff = wide_tab):\n",
    "    global df\n",
    "    i = 0 \n",
    "    while i == 0:\n",
    "        ## Get a unique list of ID\n",
    "        aa = wide_tab.copy()\n",
    "        pd_id = aa.drop_duplicates(subset = 'ID', keep = 'first')\n",
    "        pd_id = pd_id[['ID', 'Type']]\n",
    "\n",
    "        ##Create random number between 0 and 1\n",
    "\n",
    "        pd_id['boogie'] = (np.random.rand(len(pd_id)))\n",
    "        pd_id['Modeling_Group'] = np.where(pd_id['boogie'] > 0.7, 'Test', 'Train')\n",
    "        pd_id_pivot = pd_id.pivot_table(index = 'Type', columns = 'Modeling_Group', aggfunc = 'size')\n",
    "        # pd_id_pivot['Roll']['Test']\n",
    "        test_values = pd_id_pivot.loc[:]['Test']\n",
    "        i = test_values[(test_values > (n_id//3)*0.9) & (test_values < (n_id //3) * 1.1)].shape[0] // 3\n",
    "\n",
    "    pd_id = pd_id.drop(['Type'], axis = 1)\n",
    "    X_data = pd.merge(dff, pd_id, on = 'ID', how = 'left')\n",
    "    #print(X_data.head())\n",
    "    X_train = X_data[X_data['Modeling_Group'] == 'Train']\n",
    "    y_train = X_train['Type']\n",
    "    X_train = X_train.drop(['Type','boogie', 'Modeling_Group', 'index', 'Interval Time', 'Time', 'lets_diff'], axis = 1, errors='ignore')\n",
    "\n",
    "    X_test = X_data[X_data['Modeling_Group'] == 'Test']\n",
    "    y_test = X_test['Type']\n",
    "    X_test = X_test.drop(['Type', 'boogie', 'Modeling_Group', 'index', 'Interval Time', 'Time', 'lets_diff'], axis = 1, errors = 'ignore')\n",
    "\n",
    "    return X_data, X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 This function is to quiclkly evaluate the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_tab(alg, X_data, X_train, y_train, X_test, y_test, accuracy_return : bool = False, plot : bool = False, tree = False):\n",
    "    y_test_ID = X_data[X_data['Modeling_Group'] == 'Test'][['ID', 'Type']]\n",
    "    X_train = X_train.drop(['ID', 'lets_tab'], axis = 1)\n",
    "    X_test = X_test.drop(['ID', 'lets_tab'], axis = 1)\n",
    "    \n",
    "    clf = alg.fit(X_train, y_train)\n",
    "    #Predict training set:\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, prediction)}\")\n",
    "    y_test_ID = X_data[X_data['Modeling_Group'] == 'Test'][['ID', 'Type']]\n",
    "    y_test_ID['Prediction'] = prediction\n",
    "    Y_result = y_test_ID.groupby('ID').agg(lambda x: pd.Series.mode(x)[0])\n",
    "    if plot and tree:\n",
    "        feat_imp = pd.Series(clf.feature_importances_, index = X_train.columns).sort_values(ascending = False)[:15]\n",
    "        feat_imp.plot.bar()\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        plt.show()\n",
    "    score = round(clf.score(X_test, y_test),2)\n",
    "    print(f\"Dumb Accuracy: {score}\")\n",
    "    print(f\"Adapted Accuracy: {accuracy_score(Y_result['Type'], Y_result['Prediction']):.2f}\")\n",
    "    if accuracy_return:\n",
    "        return round(accuracy_score(Y_result['Type'], Y_result['Prediction']),2)\n",
    "    return clf, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. An example with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[656  29  48  54]\n",
      " [ 63 892  52  52]\n",
      " [ 41  68 836  84]\n",
      " [ 32  31  49 936]]\n",
      "Dumb Accuracy: 0.85\n",
      "Adapted Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "random_state = 99\n",
    "#dtc as decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state = 99)\n",
    "\n",
    "## essayer avec Naive Bayes, KNN, and Logistic Regresssion\n",
    "X_data, X_train, y_train, X_test, y_test = get_split_train_test(wide_tab_diff)\n",
    "rf_tab_micro, score = evaluate_model_tab(dtc, X_data, X_train, y_train, X_test, y_test, accuracy_return=False, plot = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 uncomment cell below to save in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'WPC_test_acceleration'\n",
    "# with open(f\"classifiers/{filename}.pkl\", 'wb') as file:\n",
    "#       pickle.dump(dtc, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95 % without taking neutral into account, 85% with neutral\n",
    " \n",
    "adapted accuracy goes from 99% to 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI part - Ignore it if you just want to train on decision tree ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_tab_classic = wide_tab_diff.copy()\n",
    "y_c = wide_tab_classic['Type']\n",
    "wide_tab_classic = wide_tab_classic.drop(['Type', 'ID', 'Time', 'Interval Time', 'index', 'lets_tab', 'lets_diff'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI part, below there is a neural network with one hidden layer of 8 neurons and an output layer of 4 neurons\n",
    "\\\n",
    "There is a part to uncomment if you want to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     48\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Exclude the first few epochs so the graph is easier to read\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory_1\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     53\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history_1\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     54\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_1' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "mapping = {'X_acc' : 0, 'Y_acc' : 1, 'Z_acc' : 2, 'Neutral' : 3}\n",
    "\n",
    "def to_int(l):\n",
    "    return [mapping[i] for i in l]\n",
    "\n",
    "\n",
    "#split dataset in features and target variable\n",
    "X_ai, y_ai = wide_tab_classic.copy(), y_c.copy()\n",
    "X_ai, y_ai = np.array(X_ai.to_numpy()), np.array(y_ai.to_numpy())\n",
    "y_ai = np.array(to_int(y_ai))\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_ai_t = scaler.fit_transform(X_ai)\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ai ,X_test_ai ,y_train_ai ,y_test_ai =train_test_split(X_ai_t,y_ai,test_size=0.1,random_state=0)\n",
    "\n",
    "\n",
    "# We'll use Keras to create a Neural network\n",
    "model = tf.keras.Sequential()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(42,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    # keras.layers.Dense(8,activation='relu'),\n",
    "    keras.layers.Dense(4,activation='sigmoid',\n",
    "                           )\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "##UNCOMMENT BELOW IF YOU WANT TO TRAIN THE AI MODEL\n",
    "##|||\n",
    "##vvv\n",
    "\n",
    "#history_1 = model.fit(X_train_ai, y_train_ai, epochs=30, validation_data=(X_test_ai, y_test_ai))\n",
    "\n",
    "##^^^\n",
    "##|||\n",
    "##UNCOMMENT ABOVE IF YOU WANT TO TRAIN THE AI MODEL\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto'\n",
    ")\n",
    "\n",
    "# Exclude the first few epochs so the graph is easier to read\n",
    "loss = history_1.history['loss']\n",
    "val_loss = history_1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "\n",
    "\n",
    "# SKIP : number of first measures we don't want to display, for a better displaying\n",
    "SKIP = 5\n",
    "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
    "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Find the weights of the model\n",
    "weights = model.get_weights()\n",
    "print(weights)\n",
    "\n",
    "\n",
    "# Saving the array in a text file\n",
    "file = open(\"micropython_codes/ESP32/temporary_models/hyper_param_acc.txt\", \"w+\")\n",
    "content = str(weights)\n",
    "file.write(content)\n",
    "file.close()\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_ai)\n",
    "# extract the predicted probabilities\n",
    "p_pred = model.predict(X_test_ai)\n",
    "p_pred = p_pred.flatten()\n",
    "print(p_pred.round(2))\n",
    "# [1. 0.01 0.91 0.87 0.06 0.95 0.24 0.58 0.78 ...\n",
    "\n",
    "# extract the predicted class labels\n",
    "y_pred = np.where(p_pred > 0.5, 1, 0)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to get the weights to use the micropython models, copy paste them from the hyper_param_gyr.txt file inside of micropython_codes/ESP32/temporary_models and paste them in the MicroPython model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
