{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.basename(os.getcwd()) != \"ML_FILE\":\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Loads accelerations data_set - adds yours data files here to load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldfC1 = pd.read_csv('data_sensor/test_ori_V0_3s.csv')\n",
    "alldfC2 = pd.read_csv('data_sensor/test_ori_V0_3s.csv')\n",
    "\n",
    "alldfneutral= pd.read_csv('data_sensor/test_neutral.csv')\n",
    "alldfneutral['Type'] = 'Neutral'\n",
    "\n",
    "alldfacc = pd.read_csv('data_sensor/test_acc_C2_V0.csv')\n",
    "alldfacc['Type'] = 'Neutral'\n",
    "#add yours data files below \n",
    "\n",
    "\n",
    "#example: type newdf = pd.read_csv('data_sensor/newdf_data.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to concatenate several dataframe, while keeping consistency of IDs and indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_concat(List_df):\n",
    "    df = pd.DataFrame()\n",
    "    def lil_concat(a,b):\n",
    "        if a.empty:\n",
    "            return b\n",
    "        else:\n",
    "            b['ID'] += a['ID'].max()\n",
    "            b['index'] += a['index'].max() + 1\n",
    "            return (pd.concat([a,b], axis = 0, ignore_index=True))\n",
    "    for i in range(len(List_df)):\n",
    "        df = lil_concat(df, List_df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the dataset, just concat the list of the \"littles\" datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Get the whole dataset, please modify list data below if you want to add your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ldata = [alldfC1, alldfC2, alldfneutral, alldfacc]\n",
    "#example : do ListData.append(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44261, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldf = perfect_concat([alldfC1, alldfC2, alldfneutral, alldfacc])\n",
    "alldf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Remove the gravity part of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max index : 44260\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\" Max index : {alldf.index.max()}\")\n",
    "DEG_TO_RAD = np.pi / 180\n",
    "\n",
    "alldf['X_acc'] = alldf['X_acc'] - 9.81* np.sin(DEG_TO_RAD * alldf['Pitch'])\n",
    "alldf['Y_acc'] = alldf['Y_acc'] - 9.81 * np.sin(DEG_TO_RAD * alldf['Roll'])*np.cos(DEG_TO_RAD * alldf['Pitch'])\n",
    "alldf['Z_acc'] = alldf['Z_acc'] - 9.81 * np.cos(DEG_TO_RAD * alldf['Roll'])*np.cos(DEG_TO_RAD * alldf['Pitch'])\n",
    "\n",
    "all_data = alldf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Look on data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare number of measures by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '10007'),\n",
       " Text(0, 0, '11102'),\n",
       " Text(0, 0, '10860'),\n",
       " Text(0, 0, '12292')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = nb_by_type = all_data.groupby('Type').index.nunique().plot.barh()\n",
    "ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tabularize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define the number of previous sample considered with length_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_tab = 7\n",
    "n_id = alldf[alldf['Type'] == 'Roll'].ID.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Type', 'Interval Time', 'Time', 'index', 'X_acc_1', 'Y_acc_1', 'Z_acc_1', 'Roll_1', 'Pitch_1', 'Yaw_1', 'X_acc_2', 'Y_acc_2', 'Z_acc_2', 'Roll_2', 'Pitch_2', 'Yaw_2', 'X_acc_3', 'Y_acc_3', 'Z_acc_3', 'Roll_3', 'Pitch_3', 'Yaw_3', 'X_acc_4', 'Y_acc_4', 'Z_acc_4', 'Roll_4', 'Pitch_4', 'Yaw_4', 'X_acc_5', 'Y_acc_5', 'Z_acc_5', 'Roll_5', 'Pitch_5', 'Yaw_5', 'X_acc_6', 'Y_acc_6', 'Z_acc_6', 'Roll_6', 'Pitch_6', 'Yaw_6', 'X_acc_7', 'Y_acc_7', 'Z_acc_7', 'Roll_7', 'Pitch_7', 'Yaw_7']\n"
     ]
    }
   ],
   "source": [
    "columns = ['ID', 'Type', 'Interval Time', 'Time', 'index']\n",
    "for i in range(length_tab):\n",
    "    columns.append(f\"X_acc_{i+1}\")\n",
    "    columns.append(f\"Y_acc_{i+1}\")\n",
    "    columns.append(f\"Z_acc_{i+1}\")\n",
    "    columns.append(f\"Roll_{i+1}\")\n",
    "    columns.append(f\"Pitch_{i+1}\")\n",
    "    columns.append(f\"Yaw_{i+1}\")\n",
    "print(columns)\n",
    "wide_tab = pd.DataFrame(columns = columns)\n",
    "\n",
    "lil_tab = alldf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. We create the flipper feature : equal to 1 when the ID changes, and zero in the other case\n",
    "\\\n",
    "\"index_start\" represent the index for a same ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_tab = lil_tab.sort_values(by = ['ID', 'Time'], ascending = [True, True])\n",
    "lil_tab['flipper'] = np.where(lil_tab.ID != lil_tab.ID.shift(1), 1, 0)\n",
    "\n",
    "index_flipper = lil_tab[lil_tab['flipper'] == 1]\n",
    "index_flipper = index_flipper[['ID', 'index']]\n",
    "index_flipper.rename({'index' : 'index_start'}, axis = 1, inplace = True)\n",
    "lil_tab0 = lil_tab.merge(index_flipper, on = 'ID', how = 'left')\n",
    "lil_tab0['index_start'] = lil_tab0['index'] - lil_tab0['index_start'] #not +1 because I don't want to consider the \n",
    "#first value of each sample\n",
    "lil_tab0['lets_tab'] = np.where((lil_tab0.index_start >= length_tab), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Let's define the \"tabularized\" tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(length_tab):\n",
    "    wide_tab[f\"X_acc_{i+1}\"] = lil_tab[\"X_acc\"].shift(i)\n",
    "    wide_tab[f\"Y_acc_{i+1}\"] = lil_tab[\"Y_acc\"].shift(i)\n",
    "    wide_tab[f\"Z_acc_{i+1}\"] = lil_tab[\"Z_acc\"].shift(i)\n",
    "    wide_tab[f\"Roll_{i+1}\"] = lil_tab[\"Roll\"].shift(i)\n",
    "    wide_tab[f\"Pitch_{i+1}\"] = lil_tab[\"Pitch\"].shift(i)\n",
    "    wide_tab[f\"Yaw_{i+1}\"] = lil_tab[\"Yaw\"].shift(i)\n",
    "    \n",
    "wide_tab[\"ID\"] = lil_tab[\"ID\"]\n",
    "wide_tab[\"Type\"] = lil_tab[\"Type\"]\n",
    "wide_tab[\"Interval Time\"] = lil_tab[\"Interval Time\"]\n",
    "wide_tab[\"Time\"] = lil_tab[\"Time\"]\n",
    "wide_tab[\"index\"] = lil_tab0[\"index_start\"] - length_tab\n",
    "wide_tab[\"lets_tab\"] = lil_tab0[\"lets_tab\"]\n",
    "wide_tab = wide_tab[wide_tab[\"lets_tab\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. wide_tab_diff does only the differenciate non orienation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_tab_diff = wide_tab.copy()\n",
    "\n",
    "wide_tab_diff['lets_diff'] = np.where(wide_tab_diff.ID != wide_tab_diff.ID.shift(1), 0, 1)\n",
    "for i in range(length_tab):\n",
    "    wide_tab_diff[f\"X_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'X_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Y_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Y_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Z_acc_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Z_acc_{i+1}'], 0)\n",
    "    wide_tab_diff[f\"Roll_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Roll_{i+1}'].diff(), 0)\n",
    "    wide_tab_diff[f\"Pitch_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Pitch_{i+1}'].diff(), 0)\n",
    "    wide_tab_diff[f\"Yaw_{i+1}\"] = np.where(wide_tab_diff['lets_diff'] == 1, wide_tab_diff[f'Yaw_{i+1}'].diff(), 0)\n",
    "\n",
    "    wide_tab_diff.rename({f\"Roll_{i+1}\" : f\"diff_Roll_{i+1}\"}, axis = 1, inplace = True)\n",
    "    wide_tab_diff.rename({f\"Pitch_{i+1}\" : f\"diff_Pitch_{i+1}\"}, axis = 1, inplace = True)\n",
    "    wide_tab_diff.rename({f\"Yaw_{i+1}\" : f\"diff_Yaw_{i+1}\"}, axis = 1, inplace = True)\n",
    "wide_tab_diff = wide_tab_diff[wide_tab_diff[\"lets_diff\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. The split_train_test function is a special : it does the split on the ID and then consider all samples of the same ID to be in a same train or test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_train_test(dff = wide_tab):\n",
    "    global df\n",
    "    i = 0 \n",
    "    while i == 0:\n",
    "        ## Get a unique list of ID\n",
    "        aa = wide_tab.copy()\n",
    "        pd_id = aa.drop_duplicates(subset = 'ID', keep = 'first')\n",
    "        pd_id = pd_id[['ID', 'Type']]\n",
    "\n",
    "        ##Create random number between 0 and 1\n",
    "\n",
    "        pd_id['boogie'] = (np.random.rand(len(pd_id)))\n",
    "        pd_id['Modeling_Group'] = np.where(pd_id['boogie'] > 0.7, 'Test', 'Train')\n",
    "        pd_id_pivot = pd_id.pivot_table(index = 'Type', columns = 'Modeling_Group', aggfunc = 'size')\n",
    "        # pd_id_pivot['Roll']['Test']\n",
    "        test_values = pd_id_pivot.loc[:]['Test']\n",
    "        i = test_values[(test_values > (n_id//3)*0.8) & (test_values < (n_id //3) * 1.2)].shape[0] // 3\n",
    "\n",
    "    pd_id = pd_id.drop(['Type'], axis = 1)\n",
    "    X_data = pd.merge(dff, pd_id, on = 'ID', how = 'left')\n",
    "    #print(X_data.head())\n",
    "    X_train = X_data[X_data['Modeling_Group'] == 'Train']\n",
    "    y_train = X_train['Type']\n",
    "    X_train = X_train.drop(['Type','boogie', 'Modeling_Group', 'index', 'Interval Time', 'Time', 'lets_diff'], axis = 1, errors='ignore')\n",
    "\n",
    "    X_test = X_data[X_data['Modeling_Group'] == 'Test']\n",
    "    y_test = X_test['Type']\n",
    "    X_test = X_test.drop(['Type', 'boogie', 'Modeling_Group', 'index', 'Interval Time', 'Time', 'lets_diff'], axis = 1, errors = 'ignore')\n",
    "\n",
    "    return X_data, X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. This function is to quiclkly evaluate the model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_tab(alg, X_data, X_train, y_train, X_test, y_test, accuracy_return : bool = False, plot : bool = False, tree = False):\n",
    "    y_test_ID = X_data[X_data['Modeling_Group'] == 'Test'][['ID', 'Type']]\n",
    "    X_train = X_train.drop(['ID', 'lets_tab'], axis = 1)\n",
    "    X_test = X_test.drop(['ID', 'lets_tab'], axis = 1)\n",
    "    \n",
    "    clf = alg.fit(X_train, y_train)\n",
    "    #Predict training set:\n",
    "    prediction = clf.predict(X_test)\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, prediction)}\")\n",
    "    y_test_ID = X_data[X_data['Modeling_Group'] == 'Test'][['ID', 'Type']]\n",
    "    y_test_ID['Prediction'] = prediction\n",
    "    Y_result = y_test_ID.groupby('ID').agg(lambda x: pd.Series.mode(x)[0])\n",
    "    if plot and tree:\n",
    "        feat_imp = pd.Series(clf.feature_importances_, index = X_train.columns).sort_values(ascending = False)[:15]\n",
    "        feat_imp.plot.bar()\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        plt.show()\n",
    "    score = round(clf.score(X_test, y_test),2)\n",
    "    print(f\"Dumb Accuracy: {score}\")\n",
    "    print(f\"Adapted Accuracy: {accuracy_score(Y_result['Type'], Y_result['Prediction']):.2f}\")\n",
    "    if accuracy_return:\n",
    "        return round(accuracy_score(Y_result['Type'], Y_result['Prediction']),2)\n",
    "    return clf, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. An example with  Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[1036   48   62  103]\n",
      " [  20 2877   16   32]\n",
      " [  76   84 2598   70]\n",
      " [ 106   20   28 2740]]\n",
      "Dumb Accuracy: 0.93\n",
      "Adapted Accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "random_state = 99\n",
    "knn = KNN()\n",
    "dtc = DecisionTreeClassifier(random_state = 99)\n",
    "clf = RandomForestClassifier(n_estimators = 1, random_state = 99)\n",
    "\n",
    "X_data, X_train, y_train, X_test, y_test = get_split_train_test(wide_tab_diff)\n",
    "rf_tab_micro, score = evaluate_model_tab(dtc, X_data, X_train, y_train, X_test, y_test, accuracy_return=False, plot = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the prediction score is around 90 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. uncomment cell below to save in a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'WPC_test_orientation'\n",
    "# with open(f\"classifiers/{filename}.pkl\", 'wb') as file:\n",
    "#       pickle.dump(dtc, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "## 4. AI part - Ignore if you just want to train on classics ml models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_tab_classic = wide_tab_diff.copy()\n",
    "y_c = wide_tab_classic['Type']\n",
    "wide_tab_classic = wide_tab_classic.drop(['Type', 'ID', 'Time', 'Interval Time', 'index', 'lets_tab', 'lets_diff'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI part, below there is a neural network with one hidden layer of 8 neurons and an output layer of 4 neurons\n",
    "\\\n",
    "There is a part to uncomment if you want to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     42\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Exclude the first few epochs so the graph is easier to read\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mhistory_1\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     47\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history_1\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     48\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_1' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "mapping = {'Roll' : 0, 'Pitch' : 1, 'Yaw' : 2, 'Neutral' : 3}\n",
    "    \n",
    "def to_int(l):\n",
    "    return [mapping[i] for i in l]\n",
    "\n",
    "#split dataset in features and target variable\n",
    "X_ai, y_ai = wide_tab_classic.copy(), y_c.copy()\n",
    "X_ai, y_ai = np.array(X_ai.to_numpy()), np.array(y_ai.to_numpy())\n",
    "y_ai = np.array(to_int(y_ai))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_ai_t = scaler.fit_transform(X_ai)\n",
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ai ,X_test_ai ,y_train_ai ,y_test_ai =train_test_split(X_ai_t,y_ai,test_size=0.1,random_state=0)\n",
    "# We'll use Keras to create a Neural network\n",
    "model = tf.keras.Sequential()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(42,)),\n",
    "    keras.layers.Dense(8, activation='relu'),\n",
    "    # keras.layers.Dense(8,activation='relu'),\n",
    "    keras.layers.Dense(4,activation='sigmoid',\n",
    "                           )\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "##10 epochs only to avoid losing to much time when running the whole jupyter, 100 in normal time\n",
    "\n",
    "\n",
    "##UNCOMMENT BELOW IF YOU WANT TO TRAIN THE AI MODEL\n",
    "\n",
    "#history_1 = model.fit(X_train_ai, y_train_ai, epochs=10, validation_data=(X_test_ai, y_test_ai))\n",
    "\n",
    "##UNCOMMENT ABOVE IF YOU WANT TO TRAIN THE AI MODEL\n",
    "\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto'\n",
    ")\n",
    "\n",
    "# Exclude the first few epochs so the graph is easier to read\n",
    "loss = history_1.history['loss']\n",
    "val_loss = history_1.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "SKIP = 10\n",
    "plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')\n",
    "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Find the weights of the model\n",
    "weights = model.get_weights()\n",
    "print(weights)\n",
    "# Saving the array in a text file\n",
    "file = open(\"micropython_codes/ESP32/temporary_models/hyper_param_gyr.txt\", \"w+\")\n",
    "content = str(weights)\n",
    "file.write(content)\n",
    "file.close()\n",
    "y_pred = model.predict(X_test_ai)\n",
    "# extract the predicted probabilities\n",
    "p_pred = model.predict(X_test_ai)\n",
    "p_pred = p_pred.flatten()\n",
    "\n",
    "print(p_pred.round(2))\n",
    "# [1. 0.01 0.91 0.87 0.06 0.95 0.24 0.58 0.78 ...\n",
    "\n",
    "# extract the predictedch class labels\n",
    "y_pred = np.where(p_pred > 0.5, 1, 0)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to get the weights to use the micropython models, copy paste them from the hyper_param_gyr.txt file inside of micropython_codes/ESP32/temporary_models and paste them in the MicroPython model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
